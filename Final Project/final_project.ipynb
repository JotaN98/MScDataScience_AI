#%% md
# Imports
#%%
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, train_test_split

import warnings
warnings.filterwarnings("ignore")
#%% md
# Loading Dataset
#%%
import os
import kagglehub

# Download the dataset (it will go to cache)
path = kagglehub.dataset_download("berkerisen/wind-turbine-scada-dataset")

print("Dataset downloaded to:", path)

# List files in the directory
files = os.listdir(path)
print("Files in dataset:", files)

# Read the CSV file
csv_files = [f for f in files if f.endswith('.csv')]

if csv_files:
    csv_path = os.path.join(path, csv_files[0])
    df = pd.read_csv(csv_path)
    print(f"\nLoaded: {csv_files[0]}")
else:
    raise FileNotFoundError("No CSV file found in the dataset")
#%%
df.head()
#%%
df.index = pd.to_datetime(df['Date/Time'],format='%d %m %Y %H:%M')
df.sort_index(inplace=True)
df.head()
#%% md
# Descriptive Statistics
#%%
df.info()
#%%
df.describe()
#%% md
There are negative values for LV Active Power feature, need to investigate further the validity of this measurements, two scenarios:
* Wind speed is low -> blades are not spinning and turbine is consuming electricity
* Wind speed is high -> blades are spinning and probably a sensor malfunction
#%%
fig, ax = plt.subplots(1, 3, figsize=(12, 3))
sns.scatterplot(data=df[df['LV ActivePower (kW)'] < 0], x='Wind Speed (m/s)', y='LV ActivePower (kW)', ax=ax[0])
ax[0].set_title('Negative Power Values')
ax[0].axhline(0, color='red', linestyle='--')

sns.scatterplot(data=df, x='Wind Speed (m/s)', y='LV ActivePower (kW)', ax=ax[1])
ax[1].set_title('Active Power Curve')
ax[1].axhline(0, color='red', linestyle='--')

sns.scatterplot(data=df[df['Wind Speed (m/s)'] < 5], x='Wind Speed (m/s)', y='LV ActivePower (kW)', ax=ax[2])
ax[2].set_title('Active Power Curve (Wind speed < 5 m/s)')
ax[2].axhline(0, color='red', linestyle='--')

plt.xlabel("Wind Speed (m/s)")
plt.ylabel("Active Power (kW)")
plt.legend()
plt.tight_layout()
plt.show()
#%% md
We can observe that most of the negative values occur bellow the 3.5 m/s wind speed, we conclude that it is most probably not sensor malfunction and that the turbine is truly consuming more energy than it generates.Since the usual treshold for the blades to start spinning and producing electricity is around 3 m/s [FIND SOURCE]
#%% md
# Handling Outliers
#%% md
Since we are predicting the power generation output, negative values in Active Power act as noise for the model, therefore they need to be handled. In this case we set them to zero because for power production forecasting, a negative value is effectively zero power generated.
#%%
negative_values = (df['LV ActivePower (kW)'] < 0).sum()
print('Negative values in LV ActivePower (kW):',negative_values)
#%%
df.loc[df['LV ActivePower (kW)'] < 0, 'LV ActivePower (kW)'] = 0
#%% md
# Feature Extraction
#%%
#Converting to pandas date time
df['Date/Time'] = pd.to_datetime(df['Date/Time'], format ='%d %m %Y %H:%M')
#%%
#Extracting month
df['Month'] = df['Date/Time'].dt.month
#%%
#Extracting season
seasons_dict = {1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring', 6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Autumn', 10: 'Autumn', 11: 'Autumn', 12: 'Winter'}
df['Season'] = df['Month'].map(seasons_dict)
#%%
#Extracting night or day
df['night_or_day'] = np.where(
    (df['Date/Time'].dt.hour < 6) | (df['Date/Time'].dt.hour >= 18),
    'night',
    'day'
)
#%%
#Extracting day
df['Day'] = df['Date/Time'].dt.day
#%%
#Extracting hour
df['Hour']=df['Date/Time'].dt.hour
#%%
#Extracting minutes
df['Minute']=df['Date/Time'].dt.minute
#%%
# Encoding Wind Direction (Circular Feature)
# 0 and 360 are close, but the model sees them as far apart.
# We convert to sin/cos components.
import numpy as np
df['Wind_Sin'] = np.sin(np.deg2rad(df['Wind Direction (°)']))
df['Wind_Cos'] = np.cos(np.deg2rad(df['Wind Direction (°)']))
#%%
#Feature extraction completed: dropping date time column
df.drop(columns=['Date/Time'], inplace=True)
#Checking for missing values
df.isna().sum()
#%% md
# Exploratory Data Analysis
#%%
# Numerical and Date Columns
date_col = ['Month','Season','night_or_day','Day','Hour', 'Minute']
num_col = ['LV ActivePower (kW)', 'Wind Speed (m/s)', 'Theoretical_Power_Curve (KWh)', 'Wind Direction (°)']
#%% md
## Univariate Analysis
#%%
for col in num_col:
    fig, ax = plt.subplots(1, 2, figsize=(12, 3))
    sns.distplot(df[col], ax=ax[0] ,color='green')
    sns.boxplot(x=df[col], ax=ax[1])
    plt.show()
#%% md
We can find a bimodal distribution in wind direction data representing the prevailing wind direction or the two most common directions the wind blows at that specific turbine location.
#%% md
## Bivariate Analysis
#%%
sns.pairplot(df[num_col],plot_kws={'alpha': 0.2})
#%% md
* We can observe from the scatter plot between `Wind Speed` and `Theoretical Power Curve` that the wind turbine can not generate power when the wind is traveling bellow 4 m/s
* When the wind is blowing between 4 m/s and 11 m/s there is a linear growth in power and wind speed
* The generated power peaks at 3600 KWh when the wind speed passes 11 m/s
#%% md
## Correlation
#%%
sns.heatmap(df[num_col].corr(), cmap="coolwarm",annot=True, fmt=".2f", linewidths=0.4)
#%% md
## Time Plots
#%% md
### Monthly Plots
#%%
fig,axes=plt.subplots(nrows=len(num_col)//2 ,ncols=2,figsize=(11,8))
for idx,col in enumerate(df[num_col]):
    row_idx=idx//2
    col_idx=idx%2
    sns.barplot(x=df['Month'],y=df[col],data=df,ax=axes[row_idx,col_idx],errorbar=None,palette='summer')
fig.suptitle('Numerical Columns over the Months')
plt.tight_layout()
plt.show()
#%% md
### Seasonal Plots
#%%
fig,axes=plt.subplots(nrows=len(num_col)//2 ,ncols=2,figsize=(11,8))
for idx,col in enumerate(df[num_col]):
    row_idx=idx//2
    col_idx=idx%2
    sns.lineplot(x='Season',y=df[col],data=df,ax=axes[row_idx,col_idx],ci=None, palette='Greens', hue='night_or_day')
fig.suptitle('Numerical Columns over the Seasons')
plt.tight_layout()
plt.show()
#%% md
## Key Insights
* The pattern of the data shows that the active power exhibits flunctuation over the year ...

#TODO: Elaborate key insights
#%% md
# Preprocessing
#%% md
## Encoding
#%%
# One Hot Encoding Season Feature
df = pd.get_dummies(df, columns=['Season'], prefix='Season')
#%%
# Binary Encoding Night or Day Feature
df['is_day'] = df['night_or_day'].map({'day': 1, 'night': 0})

# Drop nigh_or_day feature
df.drop(columns=['night_or_day'], inplace=True)
#%% md
## Data Splitting
#%%
X = df.drop(['LV ActivePower (kW)'], axis=1)
y = df['LV ActivePower (kW)']
#%% md
Since this is time series data it is best practice to not randomly split and use time series split so the model does not have acess to future predictions

More info: https://medium.com/@Stan_DS/timeseries-split-with-sklearn-tips-8162c83612b9
#%%
tss = TimeSeriesSplit(n_splits = 3)

# We need to store scores to actually use Cross Validation, 
# otherwise we are just using the last fold.
cv_scores = []

for train_index, test_index in tss.split(X):
    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # Note: If you want to use X_train/X_test for the final "Modelling" block
    # outside this loop, you are currently only using the LAST split.
    # Usually, you would fit and score models INSIDE this loop.
    print(f"Split sizes: Train {len(X_train)}, Test {len(X_test)}")

#%%
# Visualizing the LAST split (which is what remains in memory)
y_train.groupby('Date/Time').mean().plot()
y_test.groupby('Date/Time').mean().plot()
#%% md
## Standardization
#%% md
Since Wind direction is a directional and radial/circular feature, we should not apply standardization to this column, neither to the date columns, only applying to Wind Speed and Theoretical Power Curve
#%%
from sklearn.preprocessing import StandardScaler

# Picking only the relevant columns to standardize
std_col = ['Wind Speed (m/s)', 'Theoretical_Power_Curve (KWh)']

scaler = StandardScaler()

# Fit only on training data
scaler.fit(X_train[std_col])

# Transform both train and test
X_train_scaled = scaler.transform(X_train[std_col])
X_test_scaled = scaler.transform(X_test[std_col])

# Convert back to DataFrame
X_train_scaled = pd.DataFrame(X_train_scaled, columns=std_col, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=std_col, index=X_test.index)

# Create a full copy of the original feature sets
X_train_final = X_train.copy()
X_test_final = X_test.copy()

# Overwrite the raw columns with the scaled versions
X_train_final[std_col] = X_train_scaled
X_test_final[std_col] = X_test_scaled
#%%
print(X_train_final.head())
#%%
''' can also apply
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('std_scaler', StandardScaler(), std_col)
    ],
    remainder='passthrough',
    verbose_feature_names_out=False # Keeps your column names clean
).set_output(transform="pandas")
'''
#%% md
# Modelling
#%% md
We choose to use Linear Regression, Random Forest Regression and XGBoost algorithms
#%%
from sklearn.metrics import r2_score,mean_squared_error

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
#%%
model_name=[]
r2score=[]
rmse=[]
models=[
    RandomForestRegressor(random_state=42),
    LinearRegression(),
    XGBRegressor(random_state=42),
]

for model in models:
    model.fit(X_train_final, y_train)
    y_pred = model.predict(X_test_final)
    model_name.append(model.__class__.__name__)
    r2score.append(str(r2_score( y_test , y_pred ) * 100 ))
    rmse.append(str(mean_squared_error( y_test , y_pred)))
#%% md
## Model Scores
#%%
models_df = pd.DataFrame({"Model-Name":model_name, "R2_score": r2score ,'RMSE':rmse})
models_df = models_df.astype({"R2_score": float, "RMSE": float})
models_df.sort_values("R2_score", ascending = False)
#%% md
## Final Model
#%% md
### Feature Importance
#%%
from sklearn.ensemble import RandomForestRegressor

#Create and train a Random Forest regressor
model=RandomForestRegressor(random_state=42)
model.fit(X_train_final,y_train)
feature_importance=model.feature_importances_

#Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature':X_train_final.columns,'Importance':feature_importance})

#Sort feature by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance',ascending=False)

feature_importance_df
#%% md
### Fine Tuning
#%%
